{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from numpy import vstack\n",
    "def lof_predict(embeddings_pipeline4, X_train, X_test):\n",
    "    # create one large dataset\n",
    "    composite = X_train + X_test\n",
    "    #composite = vstack((X_train, X_test))\n",
    "    #generalize\n",
    "    ref_corpus=[]\n",
    "    for text in composite:           \n",
    "            trans = regex.sub(lambda m: m.group().replace(m.group(),\"del-pass\"),text)\n",
    "            trans_ref = regey.sub(lambda n: n.group().replace(n.group(),\"delete\"),trans)\n",
    "             \n",
    "            ref_corpus.append(trans_ref)\n",
    "\n",
    "    #clean data\n",
    "    lemmatized_list=[]\n",
    "    for snt in ref_corpus:\n",
    "        #    print(snt)\n",
    "            tokens = nlp(snt)\n",
    "        #    for token in tokenized:\n",
    "            filtered_sentence = [w.text for w in tokens if not w.text in stopwords]\n",
    "        #                 lemm = token.lemma_ for token.text in token\n",
    "        #             lemmatized_list.append(lemm)\n",
    "            stri = ' '.join(filtered_sentence)\n",
    "            lemmatized_list.append(stri)\n",
    "    lemmatized_list2 = [nlp(text).vector for text in lemmatized_list]\n",
    "    \n",
    "    # make prediction on composite dataset\n",
    "    yhat = embeddings_pipeline4.fit_predict(lemmatized_list2)\n",
    "    \n",
    "    # return just the predictions on the test set\n",
    "    return yhat[len(X_train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load(filename = 'LocalOutlierFac.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx =np.load('tranx.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "stri='stock enquiry for stock number this stock needs to be stacked'\n",
    "lst= nlp(stri).vector\n",
    "test_X=[]\n",
    "test_X.append(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "yhat = lof_predict(loaded_model, trainx, test_X)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stri='stock enquiry for stock number this stock needs to be stacked'\n",
    "lst= nlp(stri).vector\n",
    "abc=[]\n",
    "abc.append(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load second model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model2 = load(filename = 'ElypticEnvelop.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=['remove delivery pass from staff account']\n",
    "test_X = [nlp(text).vector for text in X_test]\n",
    "\n",
    "joblib_preds = loaded_model2.predict(test_X)\n",
    "joblib_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load 3rd Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
